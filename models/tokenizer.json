"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 15000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 4, \"word_counts\": \"{\\\"hai\\\": 1, \\\"halo\\\": 1, \\\"siapa\\\": 2, \\\"namamu\\\": 1, \\\"kamu\\\": 1}\", \"word_docs\": \"{\\\"hai\\\": 1, \\\"halo\\\": 1, \\\"siapa\\\": 2, \\\"namamu\\\": 1, \\\"kamu\\\": 1}\", \"index_docs\": \"{\\\"3\\\": 1, \\\"4\\\": 1, \\\"2\\\": 2, \\\"5\\\": 1, \\\"6\\\": 1}\", \"index_word\": \"{\\\"1\\\": \\\"<OOV>\\\", \\\"2\\\": \\\"siapa\\\", \\\"3\\\": \\\"hai\\\", \\\"4\\\": \\\"halo\\\", \\\"5\\\": \\\"namamu\\\", \\\"6\\\": \\\"kamu\\\"}\", \"word_index\": \"{\\\"<OOV>\\\": 1, \\\"siapa\\\": 2, \\\"hai\\\": 3, \\\"halo\\\": 4, \\\"namamu\\\": 5, \\\"kamu\\\": 6}\"}}"