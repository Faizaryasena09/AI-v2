"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 15000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 3, \"word_counts\": \"{\\\"halo\\\": 1, \\\"siapa\\\": 2, \\\"kamu\\\": 1, \\\"namamu\\\": 1}\", \"word_docs\": \"{\\\"halo\\\": 1, \\\"siapa\\\": 2, \\\"kamu\\\": 1, \\\"namamu\\\": 1}\", \"index_docs\": \"{\\\"3\\\": 1, \\\"2\\\": 2, \\\"4\\\": 1, \\\"5\\\": 1}\", \"index_word\": \"{\\\"1\\\": \\\"<OOV>\\\", \\\"2\\\": \\\"siapa\\\", \\\"3\\\": \\\"halo\\\", \\\"4\\\": \\\"kamu\\\", \\\"5\\\": \\\"namamu\\\"}\", \"word_index\": \"{\\\"<OOV>\\\": 1, \\\"siapa\\\": 2, \\\"halo\\\": 3, \\\"kamu\\\": 4, \\\"namamu\\\": 5}\"}}"